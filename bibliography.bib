@book{MarslandStephen2015Ml:a,
series = {Chapman \& Hall/CRC machine learning & pattern recognition series},
isbn = {1466583282},
year = {2015},
title = {Machine learning : an algorithmic perspective},
edition = {Second edition.},
language = {eng},
author = {Marsland, Stephen},
keywords = {Machine learning; Algorithms},
}

@Article{Halligan2015,
author="Halligan, Steve
and Altman, Douglas G.
and Mallett, Susan",
title="Disadvantages of using the area under the receiver operating characteristic curve to assess imaging tests: A discussion and proposal for an alternative approach",
journal="European Radiology",
year="2015",
month="Apr",
day="01",
volume="25",
number="4",
pages="932--939",
abstract="The objectives are to describe the disadvantages of the area under the receiver operating characteristic curve (ROC AUC) to measure diagnostic test performance and to propose an alternative based on net benefit.",
issn="1432-1084",
doi="10.1007/s00330-014-3487-0",
url="https://doi.org/10.1007/s00330-014-3487-0"
}


@Article{Hand2009,
author="Hand, David J.",
title="Measuring classifier performance: a coherent alternative to the area under the ROC curve",
journal="Machine Learning",
year="2009",
month="Oct",
day="01",
volume="77",
number="1",
pages="103--123",
abstract="The area under the ROC curve (AUC) is a very widely used measure of performance for classification and diagnostic rules. It has the appealing property of being objective, requiring no subjective input from the user. On the other hand, the AUC has disadvantages, some of which are well known. For example, the AUC can give potentially misleading results if ROC curves cross. However, the AUC also has a much more serious deficiency, and one which appears not to have been previously recognised. This is that it is fundamentally incoherent in terms of misclassification costs: the AUC uses different misclassification cost distributions for different classifiers. This means that using the AUC is equivalent to using different metrics to evaluate different classification rules. It is equivalent to saying that, using one classifier, misclassifying a class 1 point is p times as serious as misclassifying a class 0 point, but, using another classifier, misclassifying a class 1 point is P times as serious, where p≠P. This is nonsensical because the relative severities of different kinds of misclassifications of individual points is a property of the problem, not the classifiers which happen to have been chosen. This property is explored in detail, and a simple valid alternative to the AUC is proposed.",
issn="1573-0565",
doi="10.1007/s10994-009-5119-5",
url="https://doi.org/10.1007/s10994-009-5119-5"
}

@article{landiskochkappa,
 ISSN = {0006341X, 15410420},
 URL = {http://www.jstor.org/stable/2529310},
 abstract = {This paper presents a general statistical methodology for the analysis of multivariate categorical data arising from observer reliability studies. The procedure essentially involves the construction of functions of the observed proportions which are directed at the extent to which the observers agree among themselves and the construction of test statistics for hypotheses involving these functions. Tests for interobserver bias are presented in terms of first-order marginal homogeneity and measures of interobserver agreement are developed as generalized kappa-type statistics. These procedures are illustrated with a clinical diagnosis example from the epidemiological literature.},
 author = {J. Richard Landis and Gary G. Koch},
 journal = {Biometrics},
 number = {1},
 pages = {159--174},
 publisher = {[Wiley, International Biometric Society]},
 title = {The Measurement of Observer Agreement for Categorical Data},
 volume = {33},
 year = {1977}
}

@book{JamesGareth2013Aits,
series = {Springer texts in statistics},
abstract = {"An Introduction to Statistical Learning provides an accessible overview of the field of statistical learning, an essential toolset for making sense of the vast and complex data sets that have emerged in fields ranging from biology to finance to marketing to astrophysics in the past twenty years. This book presents some of the most important modeling and prediction techniques, along with relevant applications. Topics include linear regression, classification, resampling methods, shrinkage approaches, tree-based methods, support vector machines, clustering, and more. Color graphics and real-world examples are used to illustrate the methods presented. Since the goal of this textbook is to facilitate the use of these statistical learning techniques by practitioners in science, industry, and other fields, each chapter contains a tutorial on implementing the analyses and methods presented in R, an extremely popular open source statistical software platform. Two of the authors co-wrote The Elements of Statistical Learning (Hastie, Tibshirani and Friedman, 2nd edition 2009), a popular reference book for statistics and machine learning researchers. An Introduction to Statistical Learning covers many of the same topics, but at a level accessible to a much broader audience. This book is targeted at statisticians and non-statisticians alike who wish to use cutting-edge statistical learning techniques to analyze their data. The text assumes only a previous course in linear regression and no knowledge of matrix algebra. Provides tools for Statistical Learning that are essential for practitioners in science, industry and other fields. Analyses and methods are presented in R. Topics include linear regression, classification, resampling methods, shrinkage approaches, tree-based methods, support vector machines, and clustering. Extensive use of color graphics assist the reader"--Publisher description.},
isbn = {9781461471370},
year = {2013},
title = {An introduction to statistical learning : with applications in R},
edition = {[Uncorrected edition].},
language = {eng},
author = {James, Gareth},
keywords = {Mathematical statistics; Mathematical models; Mathematical statistics -- Problems, exercises, etc; Mathematical models -- Problems, exercises, etc; R (Computer program language); Statistics; Mathematical models; Mathematical statistics; R (Computer program language); Statistics; Statistik; Maschinelles Lernen; Statistik; Statistique mathématique; Modèles mathématiques; Statistique mathématique -- Problèmes et exercices; Modèles mathématiques -- Problèmes et exercices; Problems and exercises},
lccn = {2013936251},
}

@misc{minaee_2019, title={20 Popular Machine Learning Metrics. Part 1: Classification & Regression Evaluation Metrics}, url={https://towardsdatascience.com/20-popular-machine-learning-metrics-part-1-classification-regression-evaluation-metrics-1ca3e282a2ce}, journal={Medium}, publisher={Towards Data Science}, author={Minaee, Shervin}, year={2019}, month={Oct}}