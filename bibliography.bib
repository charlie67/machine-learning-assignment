@book{MarslandStephen2015Ml:a,
series = {Chapman \& Hall/CRC machine learning & pattern recognition series},
isbn = {1466583282},
year = {2015},
title = {Machine learning : an algorithmic perspective},
edition = {Second edition.},
language = {eng},
author = {Marsland, Stephen},
keywords = {Machine learning; Algorithms},
}

@Article{Halligan2015,
author="Halligan, Steve
and Altman, Douglas G.
and Mallett, Susan",
title="Disadvantages of using the area under the receiver operating characteristic curve to assess imaging tests: A discussion and proposal for an alternative approach",
journal="European Radiology",
year="2015",
month="Apr",
day="01",
volume="25",
number="4",
pages="932--939",
abstract="The objectives are to describe the disadvantages of the area under the receiver operating characteristic curve (ROC AUC) to measure diagnostic test performance and to propose an alternative based on net benefit.",
issn="1432-1084",
doi="10.1007/s00330-014-3487-0",
url="https://doi.org/10.1007/s00330-014-3487-0"
}


@Article{Hand2009,
author="Hand, David J.",
title="Measuring classifier performance: a coherent alternative to the area under the ROC curve",
journal="Machine Learning",
year="2009",
month="Oct",
day="01",
volume="77",
number="1",
pages="103--123",
abstract="The area under the ROC curve (AUC) is a very widely used measure of performance for classification and diagnostic rules. It has the appealing property of being objective, requiring no subjective input from the user. On the other hand, the AUC has disadvantages, some of which are well known. For example, the AUC can give potentially misleading results if ROC curves cross. However, the AUC also has a much more serious deficiency, and one which appears not to have been previously recognised. This is that it is fundamentally incoherent in terms of misclassification costs: the AUC uses different misclassification cost distributions for different classifiers. This means that using the AUC is equivalent to using different metrics to evaluate different classification rules. It is equivalent to saying that, using one classifier, misclassifying a class 1 point is p times as serious as misclassifying a class 0 point, but, using another classifier, misclassifying a class 1 point is P times as serious, where p≠P. This is nonsensical because the relative severities of different kinds of misclassifications of individual points is a property of the problem, not the classifiers which happen to have been chosen. This property is explored in detail, and a simple valid alternative to the AUC is proposed.",
issn="1573-0565",
doi="10.1007/s10994-009-5119-5",
url="https://doi.org/10.1007/s10994-009-5119-5"
}

@article{landiskochkappa,
 ISSN = {0006341X, 15410420},
 URL = {http://www.jstor.org/stable/2529310},
 abstract = {This paper presents a general statistical methodology for the analysis of multivariate categorical data arising from observer reliability studies. The procedure essentially involves the construction of functions of the observed proportions which are directed at the extent to which the observers agree among themselves and the construction of test statistics for hypotheses involving these functions. Tests for interobserver bias are presented in terms of first-order marginal homogeneity and measures of interobserver agreement are developed as generalized kappa-type statistics. These procedures are illustrated with a clinical diagnosis example from the epidemiological literature.},
 author = {J. Richard Landis and Gary G. Koch},
 journal = {Biometrics},
 number = {1},
 pages = {159--174},
 publisher = {[Wiley, International Biometric Society]},
 title = {The Measurement of Observer Agreement for Categorical Data},
 volume = {33},
 year = {1977}
}
